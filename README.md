## Self-Evaluating Data Viz Agent
A reflective, feedback-driven analytics system that generates and evaluates data visualizations through an autonomous reasoning loop. Built on a reflective design pattern, the system continuously improves its output using external evaluators grounded in objective, multimodal feedback criteria.

## Key Features

‚úÖ Reflective Agentic Loop ‚Äì Self-improving code generation based on evaluator feedback<br>
‚úÖ Objective Rubric Evaluation ‚Äì Quantitative scoring for every chart iteration<br>
‚úÖ Multimodal Evaluation ‚Äì Evaluates both generated code and chart image<br>
‚úÖ LangGraph State Management ‚Äì Tracks state transitions across reflection cycles<br>
‚úÖ Sequential Feedback History ‚Äì Keeps full rubric log from all retries<br>
‚úÖ FastAPI Backend + Streamlit UI ‚Äì Lightweight, production-ready setup<br>
‚úÖ Chart Caching & Cleanup ‚Äì Serve generated charts dynamically and clear with one endpoint<br>

## Tech Stack

| Layer                      | Technology                            |
| :------------------------- | :------------------------------------ |
| **Backend Framework**      | FastAPI                               |
| **Frontend UI**            | Streamlit                             |
| **Workflow Orchestration** | LangGraph, LangChain & LangSmith      |
| **Evaluation Schema**      | Pydantic                              |
| **Visualization**          | Matplotlib, Seaborn                   |
| **Data Processing**        | Pandas                                |
| **Package & Env Manager**  | [uv](https://github.com/astral-sh/uv) |
| **Runtime**                | Python ‚â• 3.10                         |

## üé• Demo

The **Agentic BI Analyst** combines LLM reasoning with visual analytics ‚Äî turning natural language queries into polished visualizations and self-improving feedback loops.  
Below are some screenshots showcasing the workflow and UI in action.

### üß† 1. Upload & Query
Users can upload a CSV dataset, type a natural language query (e.g., *‚ÄúShow survival rate by passenger class‚Äù*), and choose the retry limit.

![Upload CSV and Query Example](assets/pic1.png)

---

### üìä 2. Generated Charts
The **Analyst LLM Agent** generates valid Python code for plotting, executes it safely, and saves the charts.  
Each iteration refines the visualization based on evaluator feedback.

![Generated Chart](assets/pic3.png)

---

### üìà 3. Evaluator Feedback (Rubric)
The **Evaluator Agent** reviews the generated chart using objective, rubric-based criteria ‚Äî  
assessing accuracy, clarity, chart type, data mapping, and more.

![Rubric Feedback Table](assets/pic2.png)

---

### üîÅ 4. Iterative Refinement
If the chart does not meet quality standards, feedback is looped back to the generator,  
leading to successive improvements ‚Äî demonstrating a **reflective design pattern**.

![Feedback Loop Visualization](assets/agent_workflow_mermaid.png)

---

## üöÄ Getting Started

Follow these steps to set up and run the application locally.

```bash
# 1Ô∏è‚É£ Clone the repository
git clone https://github.com/zufeshan12/agentic-bi-analyst.git
cd agentic-bi-analyst

# 2Ô∏è‚É£ Create and activate a virtual environment (optional but recommended)
uv venv
source .venv/bin/activate  # On Windows use: .venv\Scripts\activate

# 3Ô∏è‚É£ Install dependencies from pyproject.toml
uv sync

# 4Ô∏è‚É£ Set up environment variables
# Create a .env file in the project root and add the following:
# (Adjust according to your provider or environment)

OPENAI_API_KEY=your_openai_api_key
LANGSMITH_API_KEY=your_langsmith_api_key
LANGSMITH_ENDPOINT="https://api.smith.langchain.com"
LANGSMITH_PROJECT="BI Analyst"
LANGCHAIN_TRACING_V2=true

# 5Ô∏è‚É£ Start the FastAPI backend
uvicorn main:app --reload

# 6Ô∏è‚É£ In another terminal, run the Streamlit frontend
streamlit run app.py


------------------------------------------------------------




